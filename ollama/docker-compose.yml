services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    runtime: nvidia
    # 非 Swarm 的 CPU/記憶體限制
    cpus: "12.0"
    mem_limit: "24g"

    environment:
      OLLAMA_LLM_LIBRARY: "cuda"
      OLLAMA_MODELS: "/models"
      OLLAMA_DEBUG: "false"
      OLLAMA_KEEP_ALIVE: "5m0s"
      OLLAMA_MAX_LOADED_MODELS: "1"
      OLLAMA_MAX_QUEUE: "256"
      OLLAMA_NUM_PARALLEL: "2"
      OLLAMA_SCHED_SPREAD: "true"
      OLLAMA_ORIGINS: "*"
      OLLAMA_NUM_CTX: "4096"
      OLLAMA_GPU_LAYERS: "-1"
      NVIDIA_VISIBLE_DEVICES: "0,1,2,3,4,5"
      CUDA_DEVICE_ORDER: "PCI_BUS_ID"
      GGML_CUDA_ENABLE_UNIFIED_MEMORY: "1"

    # Swarm 才會生效；非 Swarm 會被忽略，但保留不影響
    deploy:
      resources:
        limits:
          cpus: "12.0"
          memory: "24g"
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]

    volumes:
      - /root/.ollama:/root/.ollama
      - /root/.ollama/models:/models
    ports:
      - "0.0.0.0:11434:11434"
    restart: unless-stopped

networks:
  default:
    external: true
    name: elfnet
